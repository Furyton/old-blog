<!DOCTYPE html>
<html lang="en">
    <head>
    <meta charset="utf-8">

    

    <!-- 渲染优化 -->
    <meta name="renderer" content="webkit">
    <meta name="force-rendering" content="webkit">
    <meta http-equiv="X-UA-Compatible" content="IE=Edge,chrome=1">
    <meta name="HandheldFriendly" content="True" >
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">

    <!--icon-->

    
        <link rel="shortcut icon" href="/captain-america-shield-captain-america.png">
    
    
        <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
    
    
        <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
    
    
        <link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
    
    
        <link rel="mask-icon" href="/safari-pinned-tab.svg">
    


    <!-- meta -->


<title>预训练模型学习(PTM) | Furyton</title>


    <meta name="keywords" content="Machine Learning, NLP, Pre-Trained Model, blog">




    <!-- OpenGraph -->
 
    <meta name="description" content="PTM in NLP 背景 语言表示学习 一个核心的任务是对词语进行编码(称为词嵌入)以便后续其他的NLP任务，但容易想到词语的含义依赖于语境，也就是上下文。 非上下文嵌入 早期的词嵌入方法是静态的、上下文无关的。核心的想法类似于查表，通过训练得到一个embedding矩阵，最经典的就是 Word2Vec 。模型规模较小、计算效率高，但上下文无关性以及词汇表有限都是它很大的问题。如果要用它一般得接">
<meta property="og:type" content="article">
<meta property="og:title" content="预训练模型学习(PTM)">
<meta property="og:url" content="http://furyton.github.io/2021/07/18/%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B%E5%AD%A6%E4%B9%A0-PTM/index.html">
<meta property="og:site_name" content="Furyton">
<meta property="og:description" content="PTM in NLP 背景 语言表示学习 一个核心的任务是对词语进行编码(称为词嵌入)以便后续其他的NLP任务，但容易想到词语的含义依赖于语境，也就是上下文。 非上下文嵌入 早期的词嵌入方法是静态的、上下文无关的。核心的想法类似于查表，通过训练得到一个embedding矩阵，最经典的就是 Word2Vec 。模型规模较小、计算效率高，但上下文无关性以及词汇表有限都是它很大的问题。如果要用它一般得接">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2021-07-18T17:32:03.000Z">
<meta property="article:modified_time" content="2022-04-04T14:55:13.787Z">
<meta property="article:author" content="Wu Shiguang">
<meta property="article:tag" content="Machine Learning">
<meta property="article:tag" content="NLP">
<meta property="article:tag" content="Pre-Trained Model">
<meta name="twitter:card" content="summary_large_image">


    
<link rel="stylesheet" href="/css/style/main.css">
 

    
    
    
        <link rel="stylesheet" id="hl-default-theme" href="https://cdn.jsdelivr.net/npm/highlight.js@10.1.2/styles/atom-one-light.css" media="none" >
        
            <link rel="stylesheet" id="hl-dark-theme" href="https://cdn.jsdelivr.net/npm/highlight.js@10.1.2/styles/atom-one-dark.css" media="none">
        
    

    

    
    
<link rel="stylesheet" href="/css/style/dark.css">

    
<script src="/js/darkmode.js"></script>



     
    <script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
 

    <!-- custom head -->

    
        <meta name="baidu-site-verification" content="code-X7mm2HpCMG" /> 
    
        <meta name="google-site-verification" content="MdF6kGSidGrvCpYUMmpTpg9Rxq2vUoK5C8kA5T9VxoQ" /> 
    

<meta name="generator" content="Hexo 5.4.1"><link rel="alternate" href="/atom.xml" title="Furyton" type="application/atom+xml">
</head>

    <body>
        <div id="app">
            <header class="header">
    <div class="header__left">
        <a href="/" class="button">
            <span class="logo__text">Furyton&#39;s blog</span>
        </a>
    </div>
    <div class="header__right">
        
            <div class="navbar__menus">
                
                    <a href="/" class="navbar-menu button">Main</a>
                
                    <a href="/tags/" class="navbar-menu button">Tags</a>
                
                    <a href="/archives/" class="navbar-menu button">Archives</a>
                
                    <a href="/categories/" class="navbar-menu button">Categories</a>
                
                    <a href="/about/" class="navbar-menu button">About</a>
                
                    <a href="/small-talk/" class="navbar-menu button">Talk</a>
                
            </div>
        
        
        
    <a href="/search/" id="btn-search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 1024 1024" width="24" height="24" fill="currentColor" stroke="currentColor" stroke-width="32"><path d="M192 448c0-141.152 114.848-256 256-256s256 114.848 256 256-114.848 256-256 256-256-114.848-256-256z m710.624 409.376l-206.88-206.88A318.784 318.784 0 0 0 768 448c0-176.736-143.264-320-320-320S128 271.264 128 448s143.264 320 320 320a318.784 318.784 0 0 0 202.496-72.256l206.88 206.88 45.248-45.248z"></path></svg>
    </a>


        
        
    <a href="javaScript:void(0);" id="btn-toggle-dark">
        <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path></svg>
    </a>


        
            <a class="dropdown-icon button" id="btn-dropdown" tabindex="0"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 20 20" width='24' height='24' fill="none" stroke="currentColor" stroke-width="0.7" stroke-linecap="round" stroke-linejoin="round"><path fill="currentColor" d="M3.314,4.8h13.372c0.41,0,0.743-0.333,0.743-0.743c0-0.41-0.333-0.743-0.743-0.743H3.314c-0.41,0-0.743,0.333-0.743,0.743C2.571,4.467,2.904,4.8,3.314,4.8z M16.686,15.2H3.314c-0.41,0-0.743,0.333-0.743,0.743s0.333,0.743,0.743,0.743h13.372c0.41,0,0.743-0.333,0.743-0.743S17.096,15.2,16.686,15.2z M16.686,9.257H3.314c-0.41,0-0.743,0.333-0.743,0.743s0.333,0.743,0.743,0.743h13.372c0.41,0,0.743-0.333,0.743-0.743S17.096,9.257,16.686,9.257z"></path></svg></a>
            <div class="dropdown-menus" id="dropdown-menus">
                
                    <a href="/" class="dropdown-menu button">Main</a>
                
                    <a href="/tags/" class="dropdown-menu button">Tags</a>
                
                    <a href="/archives/" class="dropdown-menu button">Archives</a>
                
                    <a href="/categories/" class="dropdown-menu button">Categories</a>
                
                    <a href="/about/" class="dropdown-menu button">About</a>
                
                    <a href="/small-talk/" class="dropdown-menu button">Talk</a>
                
            </div>
        
    </div>
</header>


            <main class="main">
    

<div class="post-title">
    <h1 class="post-title__text">
        预训练模型学习(PTM)
    </h1>
    <div class="post-title__meta">
        <a href="/archives/2021/07/" class="post-meta__date button">2021-07-18</a>
        
    <span class="separate-dot"></span><a href="/categories/Lab/" class="button">Lab</a>

    <span class="separate-dot"></span><a href="/categories/Machine-Learning/" class="button">Machine Learning</a>

    <span class="separate-dot"></span><a href="/categories/Lab/Basic/" class="button">Basic</a>

 
        
    
     
    <span id="busuanzi_container_page_pv" hidden>
        <span class="separate-dot"></span>
        <span></span>
        <span id="busuanzi_value_page_pv"></span>
        <span>Views</span>
    </span>



 

 
    </div>
</div>



<article class="post content-card">
    <div class="post__header"></div>
    <div class="post__content">
        <h1 id="ptm-in-nlp">PTM in NLP</h1>
<h2 id="背景">背景</h2>
<h3 id="语言表示学习">语言表示学习</h3>
<p>一个核心的任务是对词语进行编码(称为词嵌入)以便后续其他的NLP任务，但容易想到词语的含义依赖于语境，也就是上下文。</p>
<h4 id="非上下文嵌入">非上下文嵌入</h4>
<p>早期的词嵌入方法是静态的、上下文无关的。核心的想法类似于查表，通过训练得到一个embedding矩阵，最经典的就是 <a href="https://furyton.github.io/2020/11/11/Word2Vec/">Word2Vec</a> 。模型规模较小、计算效率高，但<strong>上下文无关性以及词汇表有限</strong>都是它很大的问题。如果要用它一般得接上一个上下文的编码器。</p>
<h4 id="上下文嵌入">上下文嵌入</h4>
<p>后来为了解决上下文问题，将静态的查表改为了动态的编码，即获得一个编码器，输入一段上下文可以得到对应的编码。各种经典的神经网络模型接踵而至，大致分为三类：基于卷积模型、序列模型和图模型。</p>
<p>前两种模型对局部信息容易掌握，但<strong>全局信息或者长期信息难以关联起来</strong>。图模型传统上是用结点代表词语，预先定义它的结构来学习，这就很依赖专家知识。后来实践上干脆直接采用全连接的图，让模型自己去学习词之间的关系，具体的是通过注意力机制去计算，而实现这一想法的代表作就是大名鼎鼎的<a href="https://furyton.github.io/2020/11/09/Transformer模型学习笔记/">变形金刚</a>。但由于它模型复杂度很高，偏差就很小，导致<strong>小数据上非常容易过拟合</strong></p>
<h2 id="为什么要预训练">为什么要预训练</h2>
<p>主要原因是数据问题，NLP中未标记数据占比太大，为了能利用它们想出的这么一个办法，来学习所谓“通用”的知识，作为下游任务的初始化部分，也可以看作是一种正则化。</p>
<h2 id="ptm需要考虑的问题">PTM需要考虑的问题</h2>
<h3 id="训练的任务">训练的任务</h3>
<p>由于是在未标记的数据上训练，所以很多PTM都是采用无监督或者是自监督。自监督大约就是自己出题自己做。我们也能看到，许多人给自己的模型出了不同的题目，随之产生的模型种类也非常多。</p>
<ul>
<li>LM (Language Model)：最常见最普通的无监督任务，就是知道前i个单词，算下一个单词的概率，通过极大似然估计来训练。</li>
<li>MLM(Masked Language Model)：就是遮住部分单词，去做完形填空，代表作就是BERT。有相当多的衍生和改进版。</li>
<li>PLM(permutation)：大致就是随机改变某些单词的位置，但假装这就是原始位置(即输入的位置编码还是原始的)，让模型再去做从左往右预测下一个单词的任务。这样模型能够随机的看到上下文的信息，不需要mask。代表作是 XLNet。看上去比较有趣。</li>
<li>DAE(Denoising autoencoder)：主要就是我给模型输入了一个认为注入了噪声的序列，我希望模型能够将噪声去除。噪声就有很多种了，比如加上Mask(这样就是MLM了)、删掉某个单词、打乱句子顺序等等。</li>
<li>CTL(Contrastive Learning)：如同我在笔记<a href="https://furyton.github.io/2020/11/11/Word2Vec/">Word2Vec</a> 中提到的，选择一个负样本作为对比来训练，这样降低了计算的复杂度。之前被用在非上下文嵌入上了，最近有新的CTL任务。
<ul>
<li>Deep Infomax ：大致就是把word2vec里的查表部分换成了用编码器。训练任务是Mask，让序列的编码和被遮挡的部分的编码尽可能地相似。</li>
<li>Replaced Token Detection：上一个任务类似，换了训练目标，预测一个单词是否被替换。</li>
<li>ELECTRA：生成器加判别器，先用MLM训练生成器，在用它初始化判别器进行训练，判别器的训练任务是判别哪些词被生成器替换了。emmm</li>
<li>Next Sentence Prediction：BERT提了这个任务，就是判断两个句子是不是连续出现的。但是又来有很多人研究发现去除/不用NSP效果会更好的🤣</li>
<li>Sentence Order Prediction：把NSP任务里的loss换掉了，作者认为是NSP融合了主题预测和连贯性预测，而前者的子任务更简单所以模型就忽视了后者。SOP是对比学习，把连续的两个句子作为正样本，两个句子顺序交换作为负样本。emmm，很合理。</li>
</ul></li>
</ul>
<h4 id="模型分析">模型分析</h4>
<ul>
<li>非上下文嵌入虽然是静态的，但他对于一些预测分类很擅长，类似于"Germany"+"captital" <span class="math inline">\(\approx\)</span> "Berlin" 。</li>
<li>对于BERT，有很多研究表明它对于句法方面的任务很不错，例如词性识别、成分标记等等，但词义等方面一般般。这被称为“<strong>语言知识</strong>”。除此之外，有部分研究发现BERT对于一些常识性的知识也还不错。“<strong>世界知识</strong>”</li>
</ul>
<h3 id="模型压缩">模型压缩</h3>
<p>PTM太大，有人考虑压缩它。</p>
<ul>
<li>剪枝。。。</li>
<li>量化(不太明白这个命名的意义)，就是降低精度。。。</li>
<li>模型共享，大概就是参数共享等等。有个比较出名的模型叫ALBERT</li>
<li>知识蒸馏，大概就是用一个小的student模型去拟合或近似大模型。这方面的研究还挺多。</li>
<li>模型替换，把PTM中较大的模块换成比较小的模块。</li>
</ul>
<h3 id="怎么用到下游任务">怎么用到下游任务</h3>
<h4 id="选择合适的ptm">选择合适的PTM</h4>
<p>不同的PTM任务会适合不同的下游任务，PTM模型的结构也多少取决于它的任务，因而也会影响在下游任务的表现。还有数据问题。</p>
<h4 id="选择合适的层">选择合适的层</h4>
<p>有人发现BERT较低的层捕捉基本的句法信息，更高的层捕获高层次的语义信息。因而不同的下游任务也可以选择不同的层来使用，比如只用静态嵌入（Word2Vec），难以捕捉高层次信息；还有使用顶层的表示；还有将所有层的表示加权一起使用的。。。</p>
<h4 id="是否微调">是否微调</h4>
<ul>
<li>特征提取：预训练模型参数被冻结。不利预迁移中间层信息。</li>
<li>微调：PTM的参数不被冻结。很多的下游任务都是采用微调。微调的方式也很多。</li>
</ul>
<h2 id="方向">方向</h2>
<ul>
<li>PTM上界：更有效的模型结构，任务等。<strong>ELECTRA</strong></li>
<li>PTM的计算复杂性优化。<strong>Transformer-XL</strong></li>
<li>模型压缩</li>
<li>更高效的微调</li>
<li>可解释性和可靠性。</li>
</ul>
<h2 id="小感悟">小感悟</h2>
<ul>
<li>模型大了，数据多了，人们说的话也变抽象了。”知识“来、”知识“去，这很抽象。暴力出奇迹，模型太大了，人都快驾驭不了了的😂</li>
<li>突然发现PTM这块是个不小的领域，有很多的突飞猛进的进展和很多待解决的问题欸。尤其是有这么多的PTM ，感觉入了个大坑。</li>
</ul>
<hr />
<p>reference：</p>
<ul>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/139479425">自然语言处理中的预训练模型（上） - 知乎 (zhihu.com)</a></li>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/142757748">自然语言处理中的预训练模型（下） - 知乎 (zhihu.com)</a></li>
<li><a target="_blank" rel="noopener" href="https://www.cnblogs.com/sandwichnlp/p/11947627.html">预训练语言模型整理（ELMo/GPT/BERT...） - 西多士NLP - 博客园 (cnblogs.com)</a></li>
</ul>

    </div>
     
    <div class="post-footer__meta"><p>updated at 2022-04-04</p></div> 
    <div class="post-entry__tags"><a href="/tags/Machine-Learning/" class="post-tags__link button"># Machine Learning</a><a href="/tags/NLP/" class="post-tags__link button"># NLP</a><a href="/tags/Pre-Trained-Model/" class="post-tags__link button"># Pre-Trained Model</a></div> 
</article>


    <div class="nav">
        <div class="nav__prev">
            
                <a href="/2021/08/05/notes-about-statistical-learning/" class="nav__link">
                    <div>
                        <svg viewBox="0 0 1024 1024" xmlns="http://www.w3.org/2000/svg" width="24" height="24"><path d="M589.088 790.624L310.464 512l278.624-278.624 45.248 45.248L400.96 512l233.376 233.376z" fill="#808080"></path></svg>
                    </div>
                    <div>
                        <div class="nav__label">
                            Previous Post
                        </div>
                        <div class="nav__title">
                            notes about statistical learning
                        </div>
                    </div>
                </a>
            
        </div>
        <div class="nav__next">
            
                <a href="/2021/06/20/%E5%87%B8%E9%9B%86/" class="nav__link">
                    <div>
                        <div class="nav__label">
                            Next Post
                        </div>
                        <div class="nav__title">
                            凸集
                        </div>
                    </div>
                    <div>
                        <svg viewBox="0 0 1024 1024" xmlns="http://www.w3.org/2000/svg" width="24" height="24"><path d="M434.944 790.624l-45.248-45.248L623.04 512l-233.376-233.376 45.248-45.248L713.568 512z" fill="#808080"></path></svg>
                    </div>
                </a>
            
        </div>
    </div>



    <div class="post__comments content-card" id="comment">
        
    <h4>Comments</h4>
    
    <div id="disqus_thread"></div>

    
    
    
    
    
    
    
    
    
    


    </div>



</main>

            <footer class="footer">
     
    <a href="#" class="button" id="b2t" aria-label="Back to Top" title="Back to Top">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 1024 1024" width="32" height="32">
            <path d="M233.376 722.752L278.624 768 512 534.624 745.376 768l45.248-45.248L512 444.128zM192 352h640V288H192z" fill="currentColor"></path>
        </svg>
    </a>

    


    
    
    
        <span id="busuanzi_container_site_uv" hidden>
            <span></span>
            <span id="busuanzi_value_site_uv"></span>
            <span>Viewers</span>
            
                <span>|</span>
            
        </span>
    
    
        <span id="busuanzi_container_site_pv" hidden>
            <span></span>
            <span id="busuanzi_value_site_pv"></span>
            <span>Views</span>
            
        </span>
    
 
 

 
    
        
                <p class="footer-copyright">
                    Copyright ©
                    2022
                        <a target="_blank" rel="noopener" href="https://github.com/Furyton">
                            Furyton
                        </a>
                </p>
                
                    
</footer>
        </div>
        
    <script defer src="https://cdn.jsdelivr.net/npm/vanilla-lazyload@17.1.0/dist/lazyload.min.js"></script>
    <script>
        window.lazyLoadOptions = {
            elements_selector: ".lazy",
            threshold: 0
        };
    </script>
 

 

 

 
    <script>
        var _hmt = _hmt || [];
        (function() {
            var hm = document.createElement('script');
            hm.src = 'https://hm.baidu.com/hm.js?00b1c580974b8695725e0ac0c3bc5826';
            var s = document.getElementsByTagName('script')[0];
            s.parentNode.insertBefore(hm, s);
        })();
    </script>
 

 



 



 


    
 


    
<script src="https://cdn.jsdelivr.net/npm/jquery@3.4.1/dist/jquery.min.js"></script>

    
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.4.1/dist/jquery.fancybox.min.css">

    
<script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.4.1/dist/jquery.fancybox.min.js"></script>

    <script>
        let lazyloadT = Boolean('true'),
            auto_fancybox = Boolean('false')
        if (auto_fancybox) {
            $(".post__content").find('img').each(function () {
                var element = document.createElement("a");
                $(element).attr("data-fancybox", "gallery");
                $(element).attr("href", $(this).attr("src"));
                if (lazyloadT) {
                    $(element).attr("href", $(this).attr("data-srcset"));
                }
                $(this).wrap(element);
            });
        } else {
            $(".post__content").find("fancybox").find('img').each(function () {
                var element = document.createElement("a");
                $(element).attr("data-fancybox", "gallery");
                $(element).attr("href", $(this).attr("src"));
                if (lazyloadT) {
                    $(element).attr("href", $(this).attr("data-srcset"));
                }
                $(this).wrap(element);
            });
        }
    </script>
 


    <script>
        if (typeof MathJax === 'undefined') {
            window.MathJax = {
                loader: {
                    source: {
                        '[tex]/amsCd': '[tex]/amscd',
                        '[tex]/AMScd': '[tex]/amscd'
                    }
                },
                tex: {
                    inlineMath: {'[+]': [['$', '$']]},
                    tags: 'ams'
                },
                options: {
                    renderActions: {
                        findScript: [10, doc => {
                            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
                                const display = !!node.type.match(/; *mode=display/);
                                const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
                                const text = document.createTextNode('');
                                node.parentNode.replaceChild(text, node);
                                math.start = {node: text, delim: '', n: 0};
                                math.end = {node: text, delim: '', n: 0};
                                doc.math.push(math);
                            });
                        }, '', false],
                        insertedScript: [200, () => {
                            document.querySelectorAll('mjx-container').forEach(node => {
                                let target = node.parentNode;
                                if (target.nodeName.toLowerCase() === 'li') {
                                    target.parentNode.classList.add('has-jax');
                                }
                            });
                        }, '', false]
                    }
                }
            };
            (function () {
                var script = document.createElement('script');
                script.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
                script.defer = true;
                document.head.appendChild(script);
            })();
        } else {
            MathJax.startup.document.state(0);
            MathJax.texReset();
            MathJax.typeset();
        }
    </script>
 

 

 

 


    
    <script type="text/javascript">

        function loadComment() {
            let utterances, i;
            (utterances = document.createElement("script")).src = 'https://utteranc.es/client.js';
            document.getElementById('disqus_thread').appendChild(utterances);
            utterances.async = true;
            utterances.setAttribute('issue-term','title')
            utterances.setAttribute('theme','github-light')
            utterances.setAttribute('repo','Furyton/Furyton.github.io')
            utterances.setAttribute('label','Comment')
            utterances.crossorigin = 'anonymous';
        }
        
        var runningOnBrowser = typeof window !== "undefined";
        var isBot = runningOnBrowser && !("onscroll" in window) || typeof navigator !== "undefined" && /(gle|ing|ro|msn)bot|crawl|spider|yand|duckgo/i.test(navigator.userAgent);
        var supportsIntersectionObserver = runningOnBrowser && "IntersectionObserver" in window;

        setTimeout(function() {
            if (!isBot && supportsIntersectionObserver) {
                var comment_observer = new IntersectionObserver(function(entries) {
                    if (entries[0].isIntersecting) {
                        loadComment();
                        comment_observer.disconnect();
                    }
                }, {
                    threshold: [0]
                });
                comment_observer.observe(document.getElementById('comment'));
            } else {
                loadComment();
            }
        }, 1);
    </script>


    
    
    

    
    
    
    
    

    
    
    
    
    

    
    
    



    </body>
</html>
