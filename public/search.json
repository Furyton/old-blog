[{"title":"about homogeneous coordinates","date":"2021-09-30T14:33:56.000Z","url":"/2021/09/30/about-homogeneous-coordinates/","tags":[["图形学","/tags/%E5%9B%BE%E5%BD%A2%E5%AD%A6/"]],"categories":[[" ",""]],"content":" 这一篇博客是很无聊的东西，随便想的，毫无意义，胡说八道。 动机：图形学课中讲到了齐次坐标，提到它对于区分点和向量的重要作用，这里我并不太明白设计它的 motivation 或者出发点。 这里我胡乱思考一下这个东西 confusion 齐次坐标是一种能够区分点和向量的一个对仿射空间中的元素的表达方式。实质上对于点和向量两者确有本质上的不同，其他书上也有这方面的讲解（我之前看过一本书的一点点，Vector Calculus, Linear Algebra, and Differerntial Forms A Unified Approach，最开始提到过这一点），点相加是没有意义的，两个点相减表示向量是合理的。至于 \\(P=\\alpha_1 P_1+ \\alpha_2 P_2+\\dots+\\alpha_n P_n\\) 本质上还是将点先转化成了向量，再做运算，这是线性代数里的思路，就是欧氏空间中的点认为是向量，这里把他看作是点可能是不同学科的定义或想法不同吧。 有点吹毛求疵、钻牛角尖了。下面开始从另一个角度解释吧。 仿射空间 这里，欧氏空间中的点，我都看作是向量了，名副其实的向量，向量空间里的向量。 假设我们有一个仿射集合 C，任取里面的一个元素 v，我们会发现 \\(C - v = \\{x-v|x\\in C\\}\\) 是个向量空间。 齐次坐标形式上是 \\((x_1,x_2,\\dots,x_n,w)\\)，\\(R^{n+1}\\) 中的东西，我们尝试赋予 w 一个含义。 设 \\(A\\) 是 \\(R^{n+1}\\) 的一个 n 维的子空间，如 \\(R^n\\)，取单位向量 v 为\\(A^\\perp\\)，即 A 的正交补（其实 v 可以随便取），那么 v + A 就是一个仿射集合。我们不妨就称这个仿射集为 C 吧。 在 \\(R^{n+1}\\) 空间中，从 v 出发，利用施密特过程，构造 n + 1 个规范正交基 (单位向量，两两正交)，它们分别是 \\(\\{v, e_1,\\dots,e_n\\}\\)，其中 \\(\\{e_1,\\dots,e_n\\}\\) 是 A 的基。任意 \\(R^{n+1}\\) 中的点都可以表示为 \\(x=\\alpha_0 v+\\alpha_1 e_1 + \\dots + \\alpha_n e_n\\)，把 \\(\\alpha_0\\) 放到最后，写成坐标形式(其实一般只有在标准基下才会有坐标)， \\(\\begin{pmatrix} \\alpha_1\\\\ \\vdots\\\\ \\alpha_n\\\\ \\alpha_0 \\end{pmatrix}\\) w 就是这里的 \\(\\alpha_0\\)，但注意，实质上我想让齐次坐标表示 C 这个仿射集合，所以 w 应当取 1 。 世界线收束 取 A 为 \\(R^n\\) （这里n=3了） 我们定义 C 中的元素为 \"点\"，A 中的元素为 \"向量\"。我们最终考虑的空间变成了 \\(R^n\\times \\{0,1\\}\\)，最后一维的 0 和 1 因此就成了 点 和 向量 的一个 indicator。这样很丑，因为一个连续的东西后面跟了一个小的离散的东西。不得已的，我们的表示允许扩展到 \\(R^{n}\\times R\\)，但还是要对最后一维做一个 normalization，也就是需要投影到我们这个丑陋的小空间中去。 舒服一点了。。。 回到开始的 confusion 部分，为啥 \\(P=\\alpha_1 P_1+ \\alpha_2 P_2+\\dots+\\alpha_n P_n\\) 它很对呢，当用上述定义代入坐标去算时，P 的 w 是 1 。 那什么又叫有意义的表达式呢（如 \\(P-2\\times V+Q\\)），算完之后在 \\(R^n\\times \\{0,1\\}\\) 中即可。 注：这里的例子 \\(P-2\\times V+Q\\) 可以分解为 \\(P-V-V+Q=(P-V)+(Q-V)\\)，若不按我们上面的定义去想似乎也能解释的通，两个点的差是向量嘛。但。。。我觉得很不好（又牛角尖了），又是交换律，又是结合律，最初的式子化到最后，过程中明显不是把它当 点 看的。 性质对应 这样把坐标中的每一位赋予含义后，一些性质变得更加自然了。 仿射变换 \\[\\begin{pmatrix} a_{1,1}&amp;a_{1,2}&amp;a_{1,3}&amp;b_1\\\\ a_{2,1}&amp;a_{2,2}&amp;a_{2,3}&amp;b_1\\\\ a_{3,1}&amp;a_{3,2}&amp;a_{3,3}&amp;b_1\\\\ 0&amp;0&amp;0&amp;1\\\\ \\end{pmatrix} \\times \\begin{pmatrix} x_1\\\\ x_2\\\\ x_n\\\\ w \\end{pmatrix} \\] \\(w=0\\) 意味着我想做 A 这个向量子空间的变换，做的就是线性变换；若 \\(w=1\\)，意味着，我想做 C 这个仿射空间的变换，得到的就是仿射变换。（其实线性变换包含于仿射变换，忽略这一点吧。。。） 关于 normalization 前面提到了扩展我们的表示到 \\(R^n\\times R\\)，然后做一个变换，这里说的很含糊，我其实也不太懂为什么齐次坐标里说好的 0 表示 向量，1 表示 点，然后又整出个 (x,y,w) 先变成 (x/w,y/w,1) 然后说表示的点是 (x/w,y/w)，但还好。 对于这一步，其实有一个有点点相关的东西叫 perspective function，\\(P:R^n\\times R_{++}\\rightarrow R^n\\) \\[ P(z,t)=z/t \\] 保凸的。 进一步的，有个 Linear-fractional function \\[ f(x)=\\frac{Ax+b}{c^Tx+d} \\] 其中 \\(A\\in R^{m\\times n}\\)，\\(dom\\; f=\\{x|c^Tx+d&gt;0\\}\\)，就是 P 跟一个仿射变换 \\(g(x)=\\begin{bmatrix} A\\\\c^T \\end{bmatrix} x + \\begin{bmatrix} b\\\\d \\end{bmatrix}\\) 的复合。 保凸的。。。 智障的总结 这真的是很无聊的东西，可能是我有点强迫症吧，不太喜欢看上去不系统化的、无根无源、的东西。比如现在的深度学习，尽管都看着很玄学，但在当下很多重要的模型中，我们都能在很早之前的文章中找到一些影子，那时还是都在做机器学习、统计学习，这就让人觉得有根可循，有可能能建立起一套理论。说多了。其实跟这个文章没啥关系。。。 虽然但是，我觉得齐次坐标还是非常方便、有用的，在几何变换上对仿射变换和线性变换做到了形式上的统一。 Update：是我浅薄了。。。 齐次坐标应该不是我之前这么理解的。。。 就和那个perspective function 一样，可以看作是对 \\(R^{n+1}\\) 空间做了一个 P 映射。记 codomain 为 B=\\(R^n\\)，domain 为 A=\\(R^{n+1}\\)，点 \\(x\\in B\\) 的齐次坐标是一个等价类 \\(\\{t\\in A|P(t)=x\\}\\) 用同一个齐次坐标表达的是同一个点，那么欧氏空间 \\(R^n\\) 中的一个无穷远处的点（必须是沿着某条线的方向无穷大的，也就是坐标间需要满足某个恒定的线性关系，到处乱飞的无穷大不可以）可以找到它的一个齐次坐标（w 置为 0），实质上是唯一的。其他点之所以做了什么normalization，是因为他们是等价的，是同一个齐次坐标。 诚如老师所讲，笛卡尔无穷远处的点称作向量，有限坐标为点。 还是很有意思的。 "},{"title":"notes about statistical learning","date":"2021-08-05T22:52:59.000Z","url":"/2021/08/05/notes-about-statistical-learning/","tags":[["Basic","/tags/Basic/"],["note","/tags/note/"],["Study","/tags/Study/"],["Machine Learning","/tags/Machine-Learning/"]],"categories":[["Lab","/categories/Lab/"],["Machine Learning","/categories/Machine-Learning/"],["Basic","/categories/Basic/"]],"content":"最近看了一点关于统计学习的知识，主要是关于knn和线性回归两种模型。两个模型算是入门级的模型了，理应是相当简单易懂的，但书中花了不小的篇幅深入地(for me)探讨了两个模型的异同，着实有趣。但有些琐碎，不得不抓紧记录下来，不然很快就忘记了[](￣▽￣)* 另外我发现我对概率统计的知识都变得很陌生了，emmm。正在恶补。 参考书是 The Elements of Statistical Learning step 1 首先是对两个模型一个简单的介绍。 Linear Models 对于一个 p 维向量的输入，\\(x^T=\\left(x_1,\\dots,x_p\\right)\\)，我们假设对应的输出与它是完全的线性关系，或者说是仿射的，即 \\(\\hat{y}=\\beta_0+x^T\\beta\\)，\\(\\beta_0\\) 是我们熟知的 bias 。一般会用更紧致的表达 \\(\\hat{y}=x^T\\beta\\)，也就是将前面的\\(\\beta_0\\) 包含进去。 非常简单的模型，几何意义也非常丰富。 去拟合它的方法很多，最常用也是最重要的一个方法就是最小二乘法(least squares), 也就是去最小化 RSS (residual sum of squares)。 幸运的是，这个问题有一个唯一的解析解(如果 \\(X^TX\\) 是非奇异的) \\(\\hat{\\beta}=(X^TX)^{-1}X^Ty\\) ，这里 \\(X\\) 是 \\(N \\times p\\) 的矩阵，注意行、列含义。 That's it! Nearest-Neighbor Methods 顾名思义，模型更加简单直接。不需要训练拟合，可以得到非常 \"非线性\" 的函数。 和它非常有关的，k=1 时，Voronoi 图。 step 2 未完 🐟"},{"title":"预训练模型学习(PTM)","date":"2021-07-18T17:32:03.000Z","url":"/2021/07/18/%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B%E5%AD%A6%E4%B9%A0-PTM/","tags":[["Machine Learning","/tags/Machine-Learning/"],["NLP","/tags/NLP/"],["Pre-Trained Model","/tags/Pre-Trained-Model/"]],"categories":[["Lab","/categories/Lab/"],["Machine Learning","/categories/Machine-Learning/"],["Basic","/categories/Lab/Basic/"]],"content":"PTM in NLP 背景 语言表示学习 一个核心的任务是对词语进行编码(称为词嵌入)以便后续其他的NLP任务，但容易想到词语的含义依赖于语境，也就是上下文。 非上下文嵌入 早期的词嵌入方法是静态的、上下文无关的。核心的想法类似于查表，通过训练得到一个embedding矩阵，最经典的就是 Word2Vec 。模型规模较小、计算效率高，但上下文无关性以及词汇表有限都是它很大的问题。如果要用它一般得接上一个上下文的编码器。 上下文嵌入 后来为了解决上下文问题，将静态的查表改为了动态的编码，即获得一个编码器，输入一段上下文可以得到对应的编码。各种经典的神经网络模型接踵而至，大致分为三类：基于卷积模型、序列模型和图模型。 前两种模型对局部信息容易掌握，但全局信息或者长期信息难以关联起来。图模型传统上是用结点代表词语，预先定义它的结构来学习，这就很依赖专家知识。后来实践上干脆直接采用全连接的图，让模型自己去学习词之间的关系，具体的是通过注意力机制去计算，而实现这一想法的代表作就是大名鼎鼎的变形金刚。但由于它模型复杂度很高，偏差就很小，导致小数据上非常容易过拟合 为什么要预训练 主要原因是数据问题，NLP中未标记数据占比太大，为了能利用它们想出的这么一个办法，来学习所谓“通用”的知识，作为下游任务的初始化部分，也可以看作是一种正则化。 PTM需要考虑的问题 训练的任务 由于是在未标记的数据上训练，所以很多PTM都是采用无监督或者是自监督。自监督大约就是自己出题自己做。我们也能看到，许多人给自己的模型出了不同的题目，随之产生的模型种类也非常多。 LM (Language Model)：最常见最普通的无监督任务，就是知道前i个单词，算下一个单词的概率，通过极大似然估计来训练。 MLM(Masked Language Model)：就是遮住部分单词，去做完形填空，代表作就是BERT。有相当多的衍生和改进版。 PLM(permutation)：大致就是随机改变某些单词的位置，但假装这就是原始位置(即输入的位置编码还是原始的)，让模型再去做从左往右预测下一个单词的任务。这样模型能够随机的看到上下文的信息，不需要mask。代表作是 XLNet。看上去比较有趣。 DAE(Denoising autoencoder)：主要就是我给模型输入了一个认为注入了噪声的序列，我希望模型能够将噪声去除。噪声就有很多种了，比如加上Mask(这样就是MLM了)、删掉某个单词、打乱句子顺序等等。 CTL(Contrastive Learning)：如同我在笔记Word2Vec 中提到的，选择一个负样本作为对比来训练，这样降低了计算的复杂度。之前被用在非上下文嵌入上了，最近有新的CTL任务。 Deep Infomax ：大致就是把word2vec里的查表部分换成了用编码器。训练任务是Mask，让序列的编码和被遮挡的部分的编码尽可能地相似。 Replaced Token Detection：上一个任务类似，换了训练目标，预测一个单词是否被替换。 ELECTRA：生成器加判别器，先用MLM训练生成器，在用它初始化判别器进行训练，判别器的训练任务是判别哪些词被生成器替换了。emmm Next Sentence Prediction：BERT提了这个任务，就是判断两个句子是不是连续出现的。但是又来有很多人研究发现去除/不用NSP效果会更好的🤣 Sentence Order Prediction：把NSP任务里的loss换掉了，作者认为是NSP融合了主题预测和连贯性预测，而前者的子任务更简单所以模型就忽视了后者。SOP是对比学习，把连续的两个句子作为正样本，两个句子顺序交换作为负样本。emmm，很合理。 模型分析 非上下文嵌入虽然是静态的，但他对于一些预测分类很擅长，类似于\"Germany\"+\"captital\" \\(\\approx\\) \"Berlin\" 。 对于BERT，有很多研究表明它对于句法方面的任务很不错，例如词性识别、成分标记等等，但词义等方面一般般。这被称为“语言知识”。除此之外，有部分研究发现BERT对于一些常识性的知识也还不错。“世界知识” 模型压缩 PTM太大，有人考虑压缩它。 剪枝。。。 量化(不太明白这个命名的意义)，就是降低精度。。。 模型共享，大概就是参数共享等等。有个比较出名的模型叫ALBERT 知识蒸馏，大概就是用一个小的student模型去拟合或近似大模型。这方面的研究还挺多。 模型替换，把PTM中较大的模块换成比较小的模块。 怎么用到下游任务 选择合适的PTM 不同的PTM任务会适合不同的下游任务，PTM模型的结构也多少取决于它的任务，因而也会影响在下游任务的表现。还有数据问题。 选择合适的层 有人发现BERT较低的层捕捉基本的句法信息，更高的层捕获高层次的语义信息。因而不同的下游任务也可以选择不同的层来使用，比如只用静态嵌入（Word2Vec），难以捕捉高层次信息；还有使用顶层的表示；还有将所有层的表示加权一起使用的。。。 是否微调 特征提取：预训练模型参数被冻结。不利预迁移中间层信息。 微调：PTM的参数不被冻结。很多的下游任务都是采用微调。微调的方式也很多。 方向 PTM上界：更有效的模型结构，任务等。ELECTRA PTM的计算复杂性优化。Transformer-XL 模型压缩 更高效的微调 可解释性和可靠性。 小感悟 模型大了，数据多了，人们说的话也变抽象了。”知识“来、”知识“去，这很抽象。暴力出奇迹，模型太大了，人都快驾驭不了了的😂 突然发现PTM这块是个不小的领域，有很多的突飞猛进的进展和很多待解决的问题欸。尤其是有这么多的PTM ，感觉入了个大坑。 reference： 自然语言处理中的预训练模型（上） - 知乎 (zhihu.com) 自然语言处理中的预训练模型（下） - 知乎 (zhihu.com) 预训练语言模型整理（ELMo/GPT/BERT...） - 西多士NLP - 博客园 (cnblogs.com) "},{"title":"凸集","date":"2021-06-20T01:32:46.000Z","url":"/2021/06/20/%E5%87%B8%E9%9B%86/","tags":[["Study","/tags/Study/"],["Baisc","/tags/Baisc/"],["convex set","/tags/convex-set/"]],"categories":[["Lab","/categories/Lab/"],["Basic","/categories/Lab/Basic/"],["Optimization","/categories/Optimization/"]],"content":" note:Your browser does not support PDF viewer, please click here to download the note manually :) --Furyton 未完待续。。。 参考书 Boyd Convex Optimization Latex 模板来自 ElegentNote from ElegantLaTex"},{"title":"Basic Topology(I)-Intuition","date":"2021-06-18T01:11:46.000Z","url":"/2021/06/18/BasicTopology(I)-Intuition/","tags":[["Basic","/tags/Basic/"],["Topology","/tags/Topology/"]],"categories":[["extend","/categories/extend/"],["Topology","/categories/extend/Topology/"]],"content":"build a basic intuition on some classic topology spaces Euler's theorem for a polyhedron P ,we have v - e + f = 2 two conditions on P need to be satisfied: any two vertices can be connected by edges on P any loop made up of straight lines divides P into two pieces 对此有两个非常有启发性的证明."},{"title":"基于微程序设计的CPU实验","date":"2021-04-10T00:57:53.000Z","url":"/2021/04/10/%E5%9F%BA%E4%BA%8E%E5%BE%AE%E7%A8%8B%E5%BA%8F%E8%AE%BE%E8%AE%A1%E7%9A%84CPU%E5%AE%9E%E9%AA%8C/","tags":[["CPU design","/tags/CPU-design/"],["experiment","/tags/experiment/"],["homework","/tags/homework/"],["microcode","/tags/microcode/"]],"categories":[["Computer Organization","/categories/Computer-Organization/"]],"content":"微指令格式 32 bit 31: HALT 30..26 : S 25 : CN 23 : CP_PC 22 : CP_MAR 21 : CP_R 20 : CP_IR 19 : R_enable 18 : R_wen 17..15 : MUX_A 14..12 : MUX_B 11..10 : MUX_R_INA 9..8 : MUX_R_INB 7..6 : MUX_R_OUT 5 : RD 4 : WE 3 : MUX_ADDR 2..0 : code 31..24 : Addr ## explanation S, CN 为 ALU的操作码 R_enable : reg_file 使能信号 R_wen ：reg_file 写信号 MUX_A, MUX_B ：A，B选择器 MUX_R_INA ：A选择器中，reg_file 选择IR中哪一个位置的寄存器号 MUX_R_INB ：B选择器中，reg_file 选择IR中哪一个位置的寄存器号 MUX_R_OUT ：ALU的输出，reg_file 选择IR中哪一个位置的寄存器号 RD ：RAM读使能 WE：RAM写使能 MUX_ADDR：选择PC、MAR作为RAM地址线 code：下地址寻址方式 ISA 参考MIPS 16bit指令集格式 指令均为单字长(16 bit)，通用寄存器有8个，均为16 bit 操作码为5位 按指令格式区分，有六类指令 OPT R1(source) R2(source) R3(destination) 5 bits 3 bits 3 bits 3 bits 2 bits ADD， R1+R2 =&gt; R3 SUB， R1-R2 =&gt; R3 MULT, R1 * R2 =&gt; R3 DIV, R1 / R2 =&gt; R3 AND, R1 &amp; R2 =&gt; R3 OR, R1 | R2 =&gt; R3 XOR, R1 ^ R2 =&gt; R3 OPT R1(source) R3(destination) 5 bits 3 bits 3 bits 3 bits 2 bits NOT, ~R1 =&gt; R3 SHL, R1&lt;&lt;1 =&gt; R3 SHR R1&gt;&gt;1 =&gt; R3 MOV, R1 =&gt; R3 LD, M[R1] =&gt; R3 ST, R1 =&gt; M[R3] OPT R1(destination) Immediate number(source) 5 bits 3 bits 8 bits MVH immediate =&gt; High_Byte(R1) MVL immediate =&gt; Low_Byte(R1) OPT R1(source) R2(destination) Immediate number 5 bits 3 bits 3 bits 5 bits ADDI R1 + immediate =&gt; R2 SUBI R1 - immediate =&gt; R2 OPT R1 5 bits 3 bits 8bits UJMP PC jumps unconditionally to R1 CJMP PC jumps to R1 when C0 = 1 ZJMP PC jumps to R1 when FZ = 1 MOVSP R1 =&gt; SP PUSH push R1 =&gt; stack[SP], SP ++ POP stack[SP] =&gt; R1, SP-- CALL run subroutine at R1 OPT 5 bits 11 bits RET return from subroutine RST 0 =&gt; PC HALT 1 =&gt; halt 与讲义中的设计不同的地方 通用寄存器数量、指令字长等均有所增加。 ALU拥有更多其他的功能，如无符号整数乘法、高（低）位byte的操作、存储A、B输入是否相等、结果是否溢出等等。 指令的功能更加丰富，包括基本的逻辑和算术运算、有（无）条件跳转、栈的操作、简单的子程序调用等。 A、B选择器并没有设计为互斥，故ALU的A、B输入端可以同时传入。 PC、MAR、通用寄存器等的时钟脉冲没有设计为互斥，可以在同一上升沿中一起进行寄存。 下地址字段的设计进行了改动，如：100表示下一个微地址为取指指令所在的微程序，简化了指令的微程序编写。 指令均为单字长指令，且大部分为寄存器寻址，每条指令的微程序大大简化。 缺点：由于指令集的精简，一些原本基本的操作可能需要1-3个指令才能实现。 CPU电路设计图 CPU核心 CPU core 控制电路（微程序控制） CU CPU整体 CPU 数据通路 通用寄存器选择方式 ALU 的输入 由于指令中有三个固定的位置上会出现寄存器的编号，因此设置了两层选择器，第一个是用来选择是启用指令中的R1、R2还是R3，第二个是A，B选择器，两个选择器都能够选择输入寄存器中的数据，即reg_files 为双端口输出。 ALU的输出 输出只需选择启用指令中的R1、R2还是R3。 说明 指令上的选择器保留了00，即寄存器000号。它被作为SP（Stack Pointer） 数据通路 reg_file: 通用寄存器组，8*16 bit，双端口输出 ALU ： 有S0...S4，CN六个控制端，用以选择运算类型 寄存器C0，FZ：分别用来保存ALU产生的进位信号和A、B相等信号 RAM：读写操作分别收WE和RE信号控制 MAR：RAM的地址寄存器 IR：指令寄存器 数据通路采用CPU内部三总线方式 寄存器之间的数据传输 \\(R1 \\overset{A 选择器}{\\rightarrow} ALU \\overset{A 直传}{\\rightarrow} R2\\) RAM 和 CPU之间的数据传输 \\(PC/MAR \\overset{Bus, RE=1}{\\rightarrow} RAM \\overset{A 选择器} ALU \\overset{A 直传}{\\rightarrow} IR\\) \\(PC/MAR \\overset{Address Bus, WE=1}{\\rightarrow} RAM, ALU \\overset{Data Bus}{\\rightarrow} RAM\\) 算术运算 \\(R1(IR) \\overset{A 选择器}{\\rightarrow} ALU, R2(IR) \\overset{B 选择器}{\\rightarrow} ALU, ALU \\overset{S, CN}{\\rightarrow} R3(IR)\\) $PC ALU PC $ 指令流程 Instruction flow 测试指令 暂无 代码 click to download"},{"title":"Linear Analysis introductory Series - Basic inequalities","date":"2021-03-17T12:17:31.000Z","url":"/2021/03/17/Linear-Analysis-introductory-Series-Basic-inequalities/","tags":[["note","/tags/note/"],["Study","/tags/Study/"],["Linear Analysis","/tags/Linear-Analysis/"],["inequalities","/tags/inequalities/"]],"categories":[["extend","/categories/extend/"],["Linear Analysis","/categories/extend/Linear-Analysis/"]],"content":"凸函数 Jensen Theorem convex function f \\[ f\\left( \\sum_{i=1}^{n}p_ix_i\\right) \\le \\sum_{i=1}^{n}f(p_ix_i) \\] \\(x_i \\in \\mathbf{dom}f\\) , \\(\\mathbf{p} \\in\\) 概率单纯形 AM-GM \\(\\sum t_i log x_i \\le log \\sum t_ix_i\\) others \\(\\varphi-mean\\) \\(\\varphi : (0,+\\infty) \\rightarrow \\mathbf{R}\\) continuous and strictly monotonic \\(\\varphi-mean\\) of a sequence \\(a\\) (\\(a_i\\gt0\\)): \\[ M_\\varphi(a)=\\varphi^{-1}\\left(\\sum_{i=1}^{n}p_i\\varphi(a_i)\\right) \\] when \\(M_\\psi\\) and \\(M_\\varphi\\) comparable? \\(\\varphi, \\psi:(0,+\\infty)\\rightarrow \\mathbf{R}\\) continuous and strictly monotonic s.t. \\(\\varphi\\psi^{-1}\\) is \\(\\left\\{\\begin{matrix} concave ,&amp; \\varphi\\;is\\;increasing\\\\ convex ,&amp; \\varphi\\;is\\;decreasing \\end{matrix}\\right.\\) \\[ M_\\varphi(a)\\le M_\\psi(a) \\] proof \\[ M_\\varphi(a)=\\varphi^{-1}\\left(\\sum_{i=1}^{n}p_i\\varphi(a_i)\\right)\\\\\\\\ = \\varphi^{-1}\\left(\\sum_{i=1}^{n}p_i\\varphi\\psi^{-1}(b_i)\\right)\\\\\\\\ \\le \\varphi^{-1}\\left(\\varphi\\psi^{-1}\\left(\\sum_{i=1}^{n}p_ib_i\\right)\\right) =M_\\psi(a) \\] for \\(\\varphi\\) is increasing special case when \\(\\varphi(t)=t^r\\;(-\\infty\\lt r \\lt +\\infty,\\; r\\ne 0)\\) , write \\(M_r\\) for \\(M_\\varphi\\) \\(M_\\infty\\) = \\(\\max_{1\\le i\\le n} a_i\\), \\(M_{-\\infty}\\) = \\(\\min_{1\\le i\\le n} a_i\\), \\(M_0\\) = \\(\\prod{a_i^{p_i}}\\), property \\(M_r\\) is continuous monotone increasing function"},{"title":"Combinatorics course note - Turan problem","date":"2021-03-06T22:17:30.000Z","url":"/2021/03/06/Combinatorics-course-note-Turan-problem/","tags":[["note","/tags/note/"],["Study","/tags/Study/"],["Graph Theory","/tags/Graph-Theory/"],["Turan problem","/tags/Turan-problem/"]],"categories":[["extend","/categories/extend/"],["Graph Theory","/categories/extend/Graph-Theory/"]],"content":"update: fix exercise ex2 课程链接 Course Link 闲来无事听的课，看看自己能坚持多久 习题解答并不能保证正确性 Turan Problem 就是求解一个数 extremal number of a graph H \\[ ex(n,H)=max\\{e(G) : |G|=n \\, and \\, G \\;is \\; H-free\\} \\] 在顶点数量限制下，不包含子图H，最大化边的数量 第一个结果是triangle-free的extremal number (Mantel 1907) \\[ e(G)\\le ex(n,K_3)= \\left \\lfloor n^2/4 \\right \\rfloor \\] 最大的图是两边各有 \\(\\left \\lfloor n/2 \\right \\rfloor\\) 个点的二分图 exercise ex1: 选择n个无理数，怎么才能\\(max\\{\\#(x_i,x_j):x_i+x_j \\in Q\\}\\) A：转化成图的问题，两个数和如果是有理数那么就有一条边，否则没有。注意到图中不能有三角形，那么根据上面的定理就解决了。 ex2: 证明对于任意一个k个顶点的树 T, \\(ex(n,T)\\le kn\\) A: 如果图G有\\(kn\\)个边，那么一定能够找到它的一个子图G'，其最小度数大于等于k。简证：从G出发一步步删点，注意到开始时平均度数等于2k。若在某一步中得到图\\(G^\\prime\\)，图中有一个点v度数小于k，那么把他去掉，得到的平均度数为 \\[ \\frac{\\sum_{u\\in G^\\prime} d_{G^\\prime}(u)-2d_{G^\\prime}(v)}{|G^\\prime|-1}\\gt \\frac{\\sum_{u\\in G^\\prime} d_{G^\\prime}(u)}{|G^\\prime|} = \\frac{2e(G^\\prime)}{|G^\\prime|}\\ge 2k\\] 这个平均值在一直增加，故最终一定可以得到所需的子图 T 可以表示成一个序列\\([v_1,v_2,\\dots,v_k]\\) ，对每个\\(v_i\\) ，只有唯一个点\\(v_j \\in [v_1,\\dots,v_{i-1}]\\) ，\\(v_i\\)和\\(v_j\\)相连。由此可以构造性的证明，上述得到的子图G'包含所有k顶点的树。 extremal structure turan r-partite turan graph =&gt; \\(T_r(n)\\) \\[ ex(n,K_{r+1})=e(T_r(n))=(1-\\frac{1}{r})\\frac{n^2}{2}-O(r), \\quad r\\in N,r\\ge2 \\] stability let \\(\\epsilon &gt; 0\\), exists \\(\\delta &gt; 0\\), G be an n-vertex \\(K_{k+1}\\) -free graph, if \\[ e(G)\\ge ex(n,K_{r+1})-\\delta n^2 \\] then G can be changed to \\(T_r(n)\\) by altering at most \\(\\epsilon n^2\\) adjacencies Motzkin-Straus \\[ f_G(x)=x^TA_Gx=\\sum_{v_iv_j\\in e(G)}x_ix_j \\] \\(\\boldsymbol{x}\\) in \\(S_n\\)， 其中 \\(S_n\\) 是概率单纯形。 \\(\\boldsymbol{x}\\) 是n维的，所以可以看作是\\(V(G)\\)上的一个权重。 有\\(f_G(\\boldsymbol{x})=2e(G_x)\\) , \\(e(G_x)\\) 是加权边的和，权重是两个顶点权重的乘积 定理内容： 图G，\\(cl(G)=k\\)，那么对于任意一个 \\(\\boldsymbol{x}\\) in \\(S_n\\) 都有一个\\(\\boldsymbol{y}\\in S_n\\)， \\[ \\left\\{\\begin{matrix} f_G(y)\\ge f_G(x)\\\\ supp(y)=K_k \\end{matrix}\\right. \\] 进一步可知，\\(f_G(\\boldsymbol{x})\\le \\frac{k-1}{k}\\) 实质上是给了一个最优化问题的解 证明略 exercise ex3: 证明 n个顶点的图\\(G\\)，两个子图\\(G_1,G_2\\) ，任取\\(\\boldsymbol{x}\\) in \\(S_n\\)， 存在\\(\\boldsymbol{y}\\in S_n\\)，满足1. \\(f_{G_i}(\\boldsymbol{y})\\ge f_{G_i}(\\boldsymbol{x})\\) ，2. \\(\\alpha(G[supp(\\boldsymbol{y})])\\le 2\\) , (max independent number) A: 只有个大概思路，没有具体去做。假设有三个点independent，鸽巢原理，至少有两个点属于一个子图，不妨设为\\(G_1\\)，做类似上述定理的证明，证明中所构造的\\(\\boldsymbol{y&#39;}\\) 恰好同样满足在\\(G_2\\)中的不等式。 ex4: 证明一个有关一类图的邻接矩阵特征值的界。图G，\\(cl(G)=k\\) ，\\(A_G\\)邻接矩阵，有 \\(\\lambda_1(A_G)^2\\le \\frac{k-1}{k}\\left \\| A \\right \\|_F^2\\)，\\(\\lambda_1\\ge \\lambda_2\\ge \\dots\\) A: 设 \\(\\lambda_1\\) 的特征向量为 \\(\\boldsymbol{y}\\; ,\\left \\| \\boldsymbol{y} \\right \\|=1\\) ，注意：\\(\\sum_{i=1}^n y_i^2=1\\)，\\(\\left \\| A \\right \\|_F^2=2e(G)\\), \\(\\lambda_1^2=f_{G_A}^2(\\boldsymbol{y})=(\\sum y_iy_j)^2\\le 2e\\sum y_i^2y_j^2\\le \\frac{k-1}{k}\\left \\| A \\right \\|_F^2\\) Erdos-Simonovits-Stone \\(\\forall H\\) \\[ ex(n,H)=\\left ( 1-\\frac{1}{\\chi(H)-1}+o(1) \\right )\\frac{n^2}{2} \\]"},{"title":"VHDL语言入门笔记","date":"2021-03-04T13:47:19.000Z","url":"/2021/03/04/VHDL%E8%AF%AD%E8%A8%80%E5%85%A5%E9%97%A8%E7%AC%94%E8%AE%B0/","tags":[["note","/tags/note/"],["VHDL","/tags/VHDL/"]],"categories":[["Programming language","/categories/Programming-language/"],["Computer Organization","/categories/Computer-Organization/"]],"content":"at the beginning the differences between VHDL and software programming languages Serial vs. Parallel concurrency: ​ VHDL: all the codes are execute at the same time. =&gt; parallel language notes basic library entity architecture a simple and_gate example saved as example_and.vhd to generate a symbol of example_and , then insert it in the diagram file File -&gt; create/update -&gt; create symbol files ... process often used for sequential logic (require a clock to operate) not common usage for combinational logic (do not require a clock) example of flip-flop saved as test.vhd component component 可以将某个完成的组件作为当前系统的一个子系统。需要在architecture内，begin前声明，在begin内具体化一个实例，同时定义好component的port所对应的signal，也就是port map。另外，各种design file都可以引用来作为一个component，Quartus提供了Create VHDL Component Declaration File功能，生成一个.cmp文件，内容是自动生成的对应component声明语句，非常方便。 example of component 参考 附件 计算机组成课程设计实验 实验一 click to download exp1 实验二 click to download exp2 实验三 click to download exp3"},{"title":"Logistic Regression and Maximum Entropy Model","date":"2021-02-15T15:56:55.000Z","url":"/2021/02/15/Logistic-regression/","tags":[["Basic","/tags/Basic/"],["Study","/tags/Study/"]],"categories":[["Lab","/categories/Lab/"],["Basic","/categories/Lab/Basic/"]],"content":"Logistic Regression 简介：一种简单的统计分类方法，因为使用了Logistic函数得名。称作回归(Regression)的原因是，这个模型实质是在做函数的拟合问题。 Logistic Function \\[ f(x)=\\frac{1}{1+e^{-x}} \\] 介于\\(0\\) ~ \\(1\\)之间 二分类的Logistic模型 \\[ P(Y=1|x)=\\frac{exp(w\\cdot x+b)}{1+exp(w\\cdot x+b)} \\] \\[ P(Y=0|x)=\\frac{1}{1+exp(w\\cdot x+b)} \\] \\(w，x \\in \\mathbf{R}^n\\) , \\(b \\in \\mathbf{R}\\) 将 \\(b\\) 并入到 \\(w\\) 中，扩充 \\(w\\) 和 \\(x\\) ，从而得到更简洁的表达 \\[ P(Y=1|x)=\\frac{exp(w\\cdot x)}{1+exp(w\\cdot x)} \\] \\[ P(Y=0|x)=\\frac{1}{1+exp(w\\cdot x)} \\] 对模型的理解 几率(odd) 记事件发生的概率为 \\(p\\) ，那么事件发生的几率为 \\(\\frac{p}{1-p}\\) 对数几率(log odd / logit 函数) \\[ logit(p)=log\\frac{p}{1-p} \\] 对于Logistic回归来说 \\[ log\\frac{P(Y=1|x)}{1-P(Y=1|x)}=w\\cdot x \\] 可以看出，这是在用线性模型来拟合logit函数，故称为“回归模型” Logistic函数 另一个角度看，Logistic函数将原本取值在实数集上的变量投影到了\\((0,1)\\) ，也就是一个概率所属的范围。 模型的求解 极大似然估计，采用数值分析的方法(梯度下降法，拟牛顿法等)求解参数向量 \\(w\\) 多分类的Logistic回归模型 \\(Y\\) 的取值范围是{1, 2, 3, ... , K} \\[ P(Y=k|x)=\\frac{exp(w_k \\cdot x)}{1+\\sum_{k=1}^{K-1}exp(w_k\\cdot x)},\\quad k=1,2,\\dots, K-1 \\] \\[ P(Y=K|x)=\\frac{1}{1+\\sum_{k=1}^{K-1}exp(w_k\\cdot x)} \\] 最大熵模型 简介：最大熵模型是根据最大熵原理得到的，简单说，在所有可能的概率模型中，熵最大的模型是最好的，因为它保留了最大的不确定性，所有不确定的部分都是接近\"等可能的\"，减小了对不确定因素的偏见。 模型的定义 利用最大熵原理，获得一个简单的分类模型。这里所求的模型是条件概率模型。根据给定的训练集，可以获得联合分布\\(P(X,Y)\\)的经验分布，以及边缘分布\\(P(X)\\)的经验分布，分别记为\\(\\tilde{P}(X,Y)\\) 和 \\(\\tilde{P}(X)\\) 。 约束 约束就是分类问题中确定的条件，也就是输入与输出之间一些已知的事实。 约束用特征函数表示。 \\[ f(x,y)=\\left\\{\\begin{matrix} 1 ,&amp; x和y满足某一事实\\\\ 0 ,&amp; 否则 \\end{matrix}\\right. \\] 如果模型能够学习到数据中的信息，那么就可以假设特征函数关于经验分布和模型分布的期望是相等的。 关于经验分布的期望 \\[ E_{\\tilde{P}}(f)=\\sum_{x,y}\\tilde{P}(x,y)f(x,y) \\] 关于模型的条件分布的期望 \\[ E_P(f)=\\sum_{x,y}\\tilde{P}(x)P(y|x)f(x,y) \\] 两者应当相等 ，即 \\[ E_{\\tilde{P}}(f)=E_P(f) \\] 另外，特征函数也可不只一个。 熵 条件熵 \\[ H(P)=-\\sum_{x,y}\\tilde{P}(x)P(y\\,|\\,x)\\,logP(y\\,|\\,x) \\] 模型学习 约束最优化问题 \\[ \\max_{P}\\quad H(P)=-\\sum_{x,y}\\tilde{P}(x)P(y\\,|\\,x)\\,logP(y\\,|\\,x) \\] \\[ s.t.\\quad E_{\\tilde{P}}(f_i)=E_P(f_i),\\ i=1,2,...,n \\] \\[ \\sum_yP(y\\,|\\,x)=1 \\] 根据拉格朗日乘子法， \\[ L(P,w)=-H(P)+w_0\\left ( 1-\\sum_yP(y\\,|\\,x)\\right)+\\sum_{i=1}^{n}w_i(E_{\\tilde{P}}(f_i)-E_P(f_i)) \\] 问题转化为 \\[ \\min_{P}\\max_{w}L(P,w) \\] 对偶问题 \\[ \\max_{w}\\min_{P}L(P,w) \\] 因为\\(L(P,w)\\) 是 P 的凸函数，故原始问题和对偶问题是等价的。(但其实两者等价的条件我还没学。。。我不懂，我不会) 通过对偶问题里面的极小化，可以获得一个含参的分布 \\[ P_w(y\\,|\\,x)=\\frac{1}{Z_w(x)}exp\\left ( \\sum_{i=1}^{n}w_if_i(x,y) \\right ) \\] \\[ Z_w(x)=\\sum_yexp\\left ( \\sum_{i=1}^{n}w_if_i(x,y) \\right ) \\] 其中\\(Z_w(x)\\) 是正规化因子，\\(w_i\\)是参数、权值。 下面在进一步去求外面的极大化就可以把参数求出来。 PS：最后一步的极大化其实等价于直接利用得到的含参的分布去做极大似然估计。 模型求解 有一个改进的迭代尺度法，计算每次参数 \\(w\\) 的增量 \\(\\delta\\) , 关注\\(L(w+\\delta)-L(w)\\) , 经过放缩确定下界， 为了得到更大的该变量。放缩是因为，原式太复杂。 拟牛顿法 "},{"title":"凸函数","date":"2021-01-27T21:25:10.000Z","url":"/2021/01/27/%E5%87%B8%E5%87%BD%E6%95%B0/","tags":[["Basic","/tags/Basic/"],["Study","/tags/Study/"],["convex function","/tags/convex-function/"]],"categories":[["Lab","/categories/Lab/"],["Basic","/categories/Lab/Basic/"],["Optimization","/categories/Optimization/"]],"content":"参考书 Boyd Convex Optimization 基本的概念和性质 什么是凸函数 \\(f:\\mathbf {R}^n \\rightarrow \\mathbf {R}, \\quad \\mathbf {dom} f\\) 是个凸集，\\(0 \\leq \\theta \\leq 1\\) \\[ f(\\theta x + (1-\\theta)y)\\leq \\theta f(x) + (1-\\theta)f(y) \\] 严格凸就是不会取到等号 凹函数就是凸函数加个负号 仿射函数是比较特殊的，它既是凸函数又是凹函数，反过来也成立。相当于凹函数和凸函数的一个共有的临界的函数。 扩展值延伸 为了让凸函数在整个实数集上有定义，在定义域外令值为无穷，具体的为正无穷\\(+\\infty\\)(相反，若为凹函数，则定义为负无穷\\(-\\infty\\)) \\[ \\tilde{f}(x)=\\left\\{\\begin{matrix} f(x) &amp;x\\in\\mathbf{dom}f\\\\ \\infty &amp; x\\notin \\mathbf{dom}f \\end{matrix}\\right. \\] 判定条件 一阶条件 假设 \\(f\\) 可微，对 \\(\\forall x,y \\in \\mathbf{dom}f\\) \\[ f(y) \\geqslant f(x) + \\bigtriangledown f(x)^T(y-x) \\] 联系了局部信息和全局信息 严格凸及凹的情况略 二阶条件 假设二阶可微，其Hessian矩阵是半正定的 未完待续。。。。"},{"title":"FFT","date":"2020-12-02T22:50:35.000Z","url":"/2020/12/02/FFT/","tags":[["Basic","/tags/Basic/"]],"categories":[[" ",""]],"content":"note : 本文仅简要地介绍FFT以及它的简单应用, 并不会过多的进行数学推导. Background 首先我们回忆一下傅里叶级数. 实数上的情形 对于一个函数\\(f(x)\\), 我们现在关注它在\\([-\\pi,\\pi]\\) 这个区间上的表现. 它能够写成(在一定条件下) \\[ f(x) = \\frac{a_0}{2} + \\sum_{k=1}^{\\infty} a_kcos(kx)+b_ksin(kx) \\] 这里 \\[ \\begin{align*} a_k = \\frac{1}{\\pi}\\int_{-\\pi}^{\\pi} f(x)cos(kx)dx \\\\\\\\ b_k = \\frac{1}{\\pi}\\int_{-\\pi}^{\\pi} f(x)sin(kx)dx \\end{align*} \\] 虽然这里我们考虑的是在区间\\([-\\pi,\\pi]\\) 上, 但现在 \\(f\\) 现在在整个\\(\\textbf{R}\\) 具有了周期性, 周期是\\(2\\pi\\). 另一方面, \\(f(x)\\) 要满足\"平方可积\", 详细来说, 所有平方可积的函数构成一个线性空间. 更进一步的有关希尔伯特空间等泛函分析的内容. 那这个线性空间自然具有一个无限维的基. 在傅里叶级数中, 选择的基是 \\[ \\\\{ 1, cos(x), sin(x), cos(2x), sin(2x), \\dots \\\\} \\] 在确定内积 \\[ \\left \\langle f,g\\right \\rangle = \\int_{-\\pi}^{\\pi}f(x)\\overline{g(x)}dx \\] 后,这组由 \\(cos(kx),sin(kx)\\dots\\) 构成的基成为了一组正交基. 当然你可以选择规范化, 去除以每个的范数. 复数上的情形 把实数上的情形推广到复数上, 这里是说\\(f(x)\\) 现在是复数函数, (x依然是实数). 我们有了更一般一点的傅里叶级数 \\[ f(x) = \\sum_{k=-\\infty}^{+\\infty} c_k e^{ikx} \\] 这里 \\[ c_k = \\frac{\\left \\langle f(x), e^{ikx} \\right\\rangle}{\\left \\|e^{ikx} \\right \\|^2} = \\frac{1}{2\\pi}\\int_{-\\pi}^{\\pi} f(x)e^{-ikx}dx \\] (注: Euler公式 \\(e^{ix}=cos(x) + i sin(x) ,\\quad i=\\sqrt{-1}\\)) 仍然是用许多的\\(cos(kx), sin(kx)\\)去表达\\(f(x)\\), 只不过变成了复数. 前面一直讲的是\\(f(x)\\)在\\([-\\pi,\\pi]\\)上的一些事情, 实际上这并不是唯一的. 现在换成区间\\([-L,L]\\) 做个映射, \\(x \\to \\pi x/L\\) 记 \\[ \\psi_k(x)=e^{i\\pi kx/L} \\] 有 \\[ c_k = \\frac{\\left \\langle f(x), \\psi_k(x) \\right\\rangle}{\\left \\|\\psi_k(x) \\right \\|^2} = \\frac{1}{2L}\\int_{-L}^{L} f(x)\\psi_k(x)dx \\] 所以 \\[ f(x) = \\sum_{k=-\\infty}^{\\infty}c_k\\psi_k(x)=\\frac{1}{2L}\\sum_{k=-\\infty}^{\\infty}\\left \\langle f(x), \\psi_k(x) \\right\\rangle \\psi_k(x) \\] 这里观察这个级数, 它实际是在将\\(f(x)\\) 分解成一系列\\(\\psi_k(x)\\) 的线性组合. 或者说一系列\\(cos(2\\pi kx/L), sin(2\\pi kx/L)\\) 的结合. 这里面\\(k\\) 蕴含着不同的正弦函数的频率. 记 \\[ \\omega_k = \\pi k/L \\\\\\\\ \\Delta \\omega = \\pi/L \\] 那么 \\[ \\begin{align*} c_k = \\frac{1}{2L} \\int_{-L}^{L} f(x)&amp; e^{-ik{\\Delta\\omega}x}dx = \\frac{1}{2L}\\int_{-L}^{L} f(x)e^{-i\\omega_kx}dx \\\\\\\\ f(x)=&amp; \\sum_{k=-\\infty}^{+\\infty}c_k e^{i\\omega_k x}=\\sum_{k=-\\infty}^{+\\infty}c_k e^{ik\\Delta \\omega x} \\end{align*} \\] 每个\\(c_k\\)都对应着一个\\(\\omega_k\\), 表示这个频率的函数所占的比重. 当\\(L \\to +\\infty\\), 也就是说\\(f(x)\\) 在\\(\\textbf{R}\\) 上非周期. 我们就得到了傅里叶变换. \\[ \\begin{align*} \\hat f(\\omega) &amp; = \\mathcal{F(f(x))}=\\int_{-\\infty}^{\\infty} f(x)e^{-i\\omega x}dx\\\\\\\\ f(x) =&amp; \\mathcal{F^{-1}(\\hat f(\\omega))} = \\frac{1}{2\\pi}\\int_{-\\infty}^{\\infty} \\hat f(\\omega)e^{i\\omega x}d\\omega \\end{align*} \\] 有趣的性质: \\[ \\begin{align*} &amp; \\mathcal{F(\\frac{d}{dx}f(x)) = i\\omega F(f(x))}\\\\\\\\\\\\\\\\ &amp; \\mathcal{F(g\\times f) = F(g)F(f)} \\end{align*} \\] DFT 对于计算机, 我们既不能方便的处理无穷级数, 也不好处理积分. 连续的情况对计算机来说比较困难. 所以出现了离散的傅里叶变换. 他的大致含义还是一样的. 对一个\\(f(x)\\)进行采样, 在0, 1, 2, ... , n - 1处的值, 分别为\\([f_0, f_1,f_2,...,f_{n-1}]\\) , 这个向量代替了原来的函数 经过傅里叶变换后的函数变成了 \\([\\hat f_0, \\hat f_1, \\hat f_2...\\hat f_{n-1}]\\) 对于 \\(\\omega\\) 的选取也有原先连续的情况变成离散. 将 \\(2\\pi\\) 平均分成了 n 份. 每一份长度是 \\(\\frac{2\\pi}{n}\\) 作为一个基本的单元. 之前的函数 \\(\\psi_k(x)\\) 成为了 \\([(e^{2\\pi ik/n})^j], j=0,1,2,...,n-1\\) 简单来说, DFT的形式就是 \\[ \\begin{align*} \\hat f_k=\\sum_{j=0}^{n-1}f_j e^{-2\\pi ijk/n}\\\\\\\\ f_k = \\frac{1}{n}\\sum_{j=0}^{n-1}\\hat f_k e^{2\\pi ijk/n} \\end{align*} \\] 注意 \\(e^{2\\pi ik/n},\\quad k=0,1,2...,n-1\\) 就是 \\(x^{n}=1\\) 的解 \\(\\psi_k = \\left[ \\begin{matrix} 1 \\\\\\\\ e^{2\\pi ik/n} \\\\\\\\ (e^{2\\pi ik/n})^2 \\\\\\\\ \\vdots \\\\\\\\ (e^{2\\pi ik/n})^{n-1} \\end{matrix} \\right]\\) 可以看作是不同频率的函数, 那么\\(\\hat f_k\\) 便可以看作是对应的\\(\\psi_k\\) 所占的大小. 首先举个简单的例子去直观的理解这个变换. 我们在函数\\(f(x)=cos(2\\pi \\times 50 x)+cos(2\\pi \\times 75x)\\) 上进行采样, 同时我们加上一个随机的噪声,范围是\\([-4,4]\\) 黑色的是原图像, 蓝色的部分是加入噪声后的图象. 当然这里的数据都是离散的, 只不过画图的时候被连在了一起.看上去像连续的. 现在我们得到了采样后的一个\\(f(x)\\)的向量, \\([f_0,f_1,f_2,...,f_{n-1}]\\) 经过DFT后, 我们能得到一个\\(\\hat f\\) 向量, 由于是复数, 这里只展示向量中每个复数的模长的平方. 可以看作是一种能量. 下面的图是经过DFT后的样子, 明显可以看到, 里面有两个非常高的点. 对应着原始的没有加入噪声的\\(cos(2\\pi \\times 50 x)\\) 和 \\(cos(2\\pi \\times 75x)\\) , 如果我们设置一个阈值, 小于100的置位0, 这样就能去除噪声. 中间的图就是对\\(\\hat f\\) 进行过滤之后再经过DFT的逆操作的得到的图象, 会发现它完全去除了噪声, 还原了最初的\\(f(x)\\) . 计算DFT 下面的问题是如何去计算DFT呢, \\[ \\begin{align*} \\hat f_k=\\sum_{j=0}^{n-1}f_j e^{-2\\pi ijk/n}\\\\\\\\ f_k = \\frac{1}{n}\\sum_{j=0}^{n-1}\\hat f_k e^{2\\pi ijk/n} \\end{align*} \\] 容易看到这实质上是一个矩阵乘法. 记 \\(\\omega_n = e^{-2\\pi i/n}\\) \\[ \\hat f=\\left [ \\begin{matrix} \\hat f_0 \\\\\\\\ \\hat f_1 \\\\\\\\ \\hat f_2 \\\\\\\\ \\vdots \\\\\\\\ \\hat f_{n-1} \\end{matrix} \\right ] = \\left [ \\begin{matrix} 1 &amp; 1 &amp; 1 &amp; 1 &amp; \\dots &amp; 1 \\\\\\\\ 1 &amp; \\omega_n &amp; \\omega_n^{2} &amp; \\omega_n^{3} &amp; \\dots &amp; \\omega_n^{n-1}\\\\\\\\ 1 &amp; \\omega_n^{2} &amp; \\omega_n^{4} &amp; \\omega_n^{6} &amp; \\dots &amp; \\omega_n^{2(n-1)} \\\\\\\\ &amp; &amp; \\vdots &amp; &amp; \\dots &amp;\\vdots \\\\\\\\ 1 &amp; \\omega_n^{n-1} &amp; \\omega_n^{2(n-1)} &amp; \\omega_n^{3(n-1)} &amp; \\dots &amp; \\omega_n^{(n-1)(n-1)} \\end{matrix}\\right] \\left[ \\begin{matrix} f_0 \\\\\\\\ f_1 \\\\\\\\ f_2 \\\\\\\\ \\vdots \\\\\\\\ f_{n-1} \\end{matrix}\\right] \\] 记这个矩阵为\\(\\mathcal{F_n}\\) , 或者称为DFT矩阵. 实质上\\(\\mathcal{F}\\) 乘上它的共轭转置 \\(\\mathcal{F^*}\\) 等于 \\(nE_n\\)， 所以它的逆矩阵是很好求的，DFT的逆变换也很容易得出. 如果直接去求这个矩阵，复杂度是比较高的\\(O(n^2)\\) 但是如果考虑到 \\(\\omega_n\\) 的特殊的性质，便能得到非常高效的\\(O(nlogn)\\) 的算法，也就是FFT FFT FFT就是一种计算DFT的高效的算法，它最初是由高斯提出. 简单来说，我们想要快速的计算矩阵\\(\\mathcal{F_n}\\) ，通过将他分解为更小的子问题，转而去求解\\(\\mathcal{F_{\\frac{n}{2}}}\\). 这里我们现在只考虑 n 是2的幂的情况. 如果n不是2的幂的话， 我们可以简单的扩充成2的幂， 并不会影响算法的效率. 不同的矩阵\\(\\mathcal{F_n}\\) 内所使用的\\(\\omega_n\\) 是不同的， 但对于n 和 n/2 我们有 \\(\\omega_n^2=\\omega_{n/2}\\) 可以轻松的进行转化。 核心的想法就是对\\([f_0, f_1,...,f_{n-1}]\\) 进行重新排序， 按照下标的奇偶进行分组. 对于多项式 \\[ \\begin{align*} p(x) =&amp; a_0+a_1x+a_2x^2+\\dots+a_{n-1}x^{n-1} \\\\\\\\ =&amp; (a_0 + a_2x^2 + a_4x^4 + \\dots + a_{n-2}x^{n-2})\\\\\\\\ &amp;+ x(a_1 + a_3x^2+a_5x^4+\\dots a_{n-1}x^{n-2})\\\\\\\\ =&amp; E(x) +xO(x) \\end{align*} \\] 同理，对于这里的DFT， 我们就会的到 \\[ \\begin{align*} \\hat f_k =&amp; \\sum_{j=0}^{n-1}f_j(\\omega_n^k)^j\\\\\\\\ =&amp; \\sum_{j=0}^{n/2-1}f_{2j}(\\omega_n^k)^{2j} +\\omega_n^k\\sum_{j=0}^{n/2-1}f_{2j+1}(\\omega_n^k)^{2j}\\\\\\\\ =&amp; \\sum_{j=0}^{n/2-1}f_{2j}(\\omega_{n/2}^k)^{j} +\\omega_n^k\\sum_{j=0}^{n/2-1}f_{2j+1}(\\omega_{n/2}^k)^{j}\\\\\\\\ =&amp; E + \\omega_n^k O \\end{align*} \\] \\(E\\) 和 \\(O\\) 便是我们的两个相同的子问题. 如果我们分别利用 \\(f_{even}\\) 和 \\(f_{odd}\\) 计算出了\\(E\\) 和 \\(O\\) 我们便能很快的根据上面的式子计算出所有的\\(\\hat f_k\\) 不仅如此，我们可以利用 \\[ \\omega_n^{k+n/2} = -\\omega_n^{k} \\] 进一步的简化. \\[ \\begin{align*} \\hat f_k=E+\\omega_n^kO\\\\\\\\ \\hat f_{k+n/2}=E-\\omega_n^kO \\end{align*} \\] 这样只需要一半的循环，就能计算出全部的\\(\\hat f_k\\) \\[ \\begin{align*} \\hat f=\\left [ \\begin{matrix} \\hat f_0 \\\\\\\\ \\hat f_1 \\\\\\\\ \\hat f_2 \\\\\\\\ \\vdots \\\\\\\\ \\hat f_{n-1} \\end{matrix} \\right ] =&amp; \\mathcal{F_n} \\left[ \\begin{matrix} f_0 \\\\\\\\ f_1 \\\\\\\\ f_2 \\\\\\\\ \\vdots \\\\\\\\ f_{n-1} \\end{matrix}\\right]\\\\\\\\ =&amp;\\left[ \\begin{matrix} E_{n/2} &amp; D_{n/2} \\\\\\\\\\\\\\\\ E_{n/2} &amp; -D_{n/2} \\end{matrix}\\right] \\left[ \\begin{matrix} \\mathcal{F_{n/2}} &amp; O \\\\\\\\\\\\\\\\ O &amp; \\mathcal{F_{n/2}} \\end{matrix}\\right] \\left[ \\begin{matrix} f_{even}\\\\\\\\\\\\\\\\f_{odd} \\end{matrix}\\right] \\end{align*} \\] 如此算法的复杂度达到了\\(O(nlogn)\\) ，如果利用并行计算，可以达到更快. FFT的应用 像前面做过的去除噪声便是一个很常见的应用 图象压缩 多项式乘法或者大整数乘法 "},{"title":"Word2Vec","date":"2020-11-11T00:03:16.000Z","url":"/2020/11/11/Word2Vec/","tags":[["Machine Learning","/tags/Machine-Learning/"],["NLP","/tags/NLP/"]],"categories":[["Lab","/categories/Lab/"],["Machine Learning","/categories/Machine-Learning/"],["Basic","/categories/Lab/Basic/"]],"content":"beginning 其实用向量来表示一个单词是很常见的, 毕竟方便一个算法或模型去表示一个单词. 但表示的方法很重要, 最完美的Embedding就是能够包含单词的语义, 相似语义的单词的表示越接近, 反之表示的向量差距越大. 下面一步步的去完成我们的目标. distance 怎么来叙述两个单词的embedding是相近的呢? 容易想到对于两个向量, 我们有一个适用于 \\(n\\) 维空间上的一个夹角公式. for \\(\\vec{a}\\), \\(\\vec{b}\\) as two embedding vectors of different words. the cosine similarity is \\[ Cosine Similarity(\\vec{a}, \\vec{b}) = \\frac{\\vec{a} \\cdot \\vec{b}}{|\\vec{a}|\\cdot|\\vec{b}|} \\] 这个评判函数相对简单, 也非常有效. 对于不同的问题, 我们也可以选择不同的距离函数. modeling embedding都是为了更好的完成一个task, 那么下面先来看一个简单的任务, 在完成任务的同时, 来获得一个适合它的embedding. predicting next word 根据语境预测下一个单词是什么. 这是一个在日常生活中非常常见的任务. 简单来做, 就是分为三步, - 输入一个单词, 先获得对应的embedding - 利用embedding去预测下一个单词的embedding - 根据embedding映射到对应的单词, 然后输出 数据集从哪来呢 一般是Wikipedia的文章等等 构造许多定长的input - output对, 做法是window sliding with a fixed length, 前面的都是input, 最后一个单词是output problem 这么做有两个问题. 预测一个单词怎么能只看前几个单词呢 最后有个 \\(n \\times d\\) , (其中 \\(n\\) 表示Vacab的大小, \\(d\\) 表示embedding的大小) , 这算起来太费时了 problem solving 取样时不仅取前面的, 把后面的也取了. 这叫skip gram. 这就成了Clozing了 换个方法, 把他改成预测谁是neighbor, 输出可能是neighbor的概率 computational problem咋整. 再调整我们的任务 我们改成更小规模的模型, 给他一个input, 和一堆output, 然后输出这些output分别是不是他的neighbor 那有可能我们训出了一个人工智障, 他告诉我们这些全都是neighbor, 我们好像也没法反驳. 给他来个negative sample, 告诉model他们不是neighbor, 这样model就成了个logistics regression模型, 规模小了很多 但咱采样的时候怎么知道他们到底有没有可能是neighbor, 我们手上的数据集肯定是不完备的呀 随机设为negative...... 问题基本解决了, 下面看具体流程 Word2Vec 我们训两个embeddings, 叫做Embedding和Context, 分别为input和一组outputs做project. 获得两组embedding: \\(\\vec{input}\\) 和 \\(\\vec{output}\\). 这个outputs肯定就是一些真正的neighbor(通过skip gram window sliding选出来的)加上一些随机的negative sample. input \\(\\to\\) Embedding \\(\\to\\) \\(\\vec{input}\\) outputs \\(\\to\\) Context \\(\\to\\) a set of \\(\\vec{output}\\)s 然后点乘, 来个softmax, 获得这么个概率, 根据误差不断训 最后, 训得差不多了, 扔掉context, 这个Embedding就是炼出来的丹 last 语义相近的那些单词, 被认为是所在的Context是相似的. 越相似的词点乘上Context, 得到的结果肯定都很接近正确结果. 这个window sliding 的length一般设成5, 越小的话得到的embedding划的越细, 就是说相近的embedding的单词所在的context几乎一样, 但要注意, 反义词很多时候也是这样, 把length设大点就能区分更多语境 End reference a blog 还没看过原paper和代码, 估计等以后了 "},{"title":"多边形的扫描转换与区域填充算法","date":"2020-11-09T22:33:08.000Z","url":"/2020/11/09/%E5%A4%9A%E8%BE%B9%E5%BD%A2%E7%9A%84%E6%89%AB%E6%8F%8F%E8%BD%AC%E6%8D%A2%E4%B8%8E%E5%8C%BA%E5%9F%9F%E5%A1%AB%E5%85%85%E7%AE%97%E6%B3%95/","tags":[["图形学","/tags/%E5%9B%BE%E5%BD%A2%E5%AD%A6/"],["数据结构","/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"]],"categories":[["Assign","/categories/Assign/"]],"content":"多边形的扫描转换与区域填充算法 为了完成这次算法作业, 需要C++ 图形编程. 我选择了OpenGL. 配置起来有点麻烦, 又遇到了一些Bug, 最后索性用了GL里面的一个库, glut.h 进行尝试. Cheating Code GLUT的学习 用的Clion, 配置了库, 但是可能有问题, 至少Glut是能正常使用的 随便找的一篇博客配置OpenGL 随便找到的一篇GLUT Tutorial 捣腾半天发现其实我需要的功能并不是很复杂, 也没用到太多glut.h更深入的东西. 了解了glut基本的结构以后还是比较容易看懂整体的流程的. main() myInit() recall to be continue... 算法代码之后再说...."},{"title":"Transformer模型学习笔记","date":"2020-11-09T01:22:25.000Z","url":"/2020/11/09/Transformer%E6%A8%A1%E5%9E%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/","tags":[["Machine Learning","/tags/Machine-Learning/"],["NLP","/tags/NLP/"]],"categories":[["Lab","/categories/Lab/"],["Machine Learning","/categories/Machine-Learning/"],["Basic","/categories/Lab/Basic/"]],"content":"Transformer 模型学习笔记 论文地址Attention Is All You Need Nice Blog for illustrating Transformer Model seq2seq with attention 提出的特点 RNN无法并行地去处理一个序列, 因为每个hidden state \\(h_i\\)都是依赖于上一个hidden state \\(h_{i-1}\\)以及input. 所以就要一个step接一个step的去循环, 对于很长的序列训练起来就很耗时. Transformer 模型利用Attention机制去捕获全局的input与output之间的依赖性, 实质上就是将整条序列看作一个input向量, 也就避免了循环神经网络中的\"循环\". 实质上算是对RNN循环过程的一个展开吧. 完全使用Attention机制, 没有使用序列对齐的循环(sequence-aligned recurrence)或者卷积层 背景知识 Self attention的优势 Gated RNNs 虽然在结构上能够记录前 \\(n - 1\\) 个token的信息, 但实际上, 随着序列变长, 最早的token信息会变得很少, 这就会失去他的准确性, 这在翻译任务中就显得非常要命, 例如, 在English-to-French的翻译里, output的第一个词大概率是依赖于input开始的部分, 这样很可能会得到很差的结果. 而transformer模型似乎是靠着更大的存储和算力来强行将前 \\(n\\) 个token利用attention融合起来. 这样看来, 似乎是对症下药, 实验上也得到了很好的结果. 而具体的self attention会在模型架构中介绍 Word Embedding Blog for introducing Word2Vec 我自己的对Word2Vec的学习笔记 模型架构 其实Transformer模型的改进应该去对比seq2seq with attention模型 首先在seq2seq模型里,用的是最初的encoder-decoder思想, 拿翻译任务来说, input就是一个句子, encoder需要一个单词一个单词(token)的去encode, 也就是扔进一个RNNs, 每次得到一个hidden state, 句子全输进去了, 最后得到的hidden state就可以作为整个句子的representation. 这个思想很简单, 给我的是不定长的, 那我就把他搞成一个固定长度的hidden state. 之后拿着这个representation当作decoder的input, 再一个单词一个单词的预测. 前后都是RNN. 乍一看貌似还是挺好的, 但问题是RNN的记忆机制和梯度问题解决的不是那么好, 句子长了前面的单词他就忘记了. 而且翻译这个任务也确实需要一种attention ,output的某一个部分会很大程度依赖于input中的一部分. 加了attention的seq2seq似乎就考虑到了这种依赖关系, attention机制也很好的做到了这一点. 至于整体做法, 上面不是说encoder内不断地生成hidden state吗, 那我们就把它们全都取出来作为decoder的input, 这样就不用太担心记忆的问题了, 毕竟你把它们都拿出来了. 然后每个output预测值会利用attention机制去给这些hidden state附上注意力的权重, 来更好地完成任务. 再到transformer. 这样纵向的来看, 似乎改进的地方确实如原论文所讲, 去掉了所有的循环连接. 完全用attention来解决. 这样做就需要在一些地方进行调整. Attention 虽然字面上讲的attention, 似乎是个很熟悉的概念, 但实际在具体的实现上是另一种更加抽象的机制. 一般来讲,这个问题是想输入两种序列, \\(S= \\{S_1, S_2, S_3 \\dots S_n \\}\\) 以及 \\(T =\\{T_1, T_2, T_3 \\dots T_m \\}\\) 我想输出\\(T\\) 对于\\(S\\) 的attention, 具体的可以说就是一个function, \\(Attention_{S_i}(T_j),\\quad i= 1,2\\dots n\\) ,然后有 \\[ \\sum_{j} Attention_{S_i}(T_j)=1, \\quad i = 1,2,\\dots n \\] 就可以,值越大就越重要. 那么整个T对于 \\(S_i\\) 的representation就是 \\(T_{s_i} = \\sum_{j} Attention_{S_i} (T_j)\\times Value(T_j)\\) 这么一个加权平均 这里的元素就是一些embedding, 一些向量. Attention的求法类似于一种查询. 每个 \\(S_i\\) 都会对应一个query向量 \\(\\boldsymbol{q_i}\\) 每个 \\(T_i\\) 又对应一个键值 \\(\\boldsymbol{k_i}\\) 以供\"查询\", 查到的结果就是两个向量的点积 \\(a_{ij} = \\boldsymbol{q_i} \\cdot \\boldsymbol{k_j}\\) , (假设这里的两个向量的维数都是 \\(d_k\\) ). 最后的Attention就是再加上一个softmax \\[ Attention_{S_i}(T_j) = \\frac{e^{\\boldsymbol{q_i} \\cdot \\boldsymbol{k_j}^T}}{\\sum_j e^{\\boldsymbol{q_i} \\cdot \\boldsymbol{k_j}^T}} \\] 每个 \\(T_j\\) 又会对应一个Value向量 \\(v_j\\) (维度可以和前面两个向量不同, 记为\\(d_v\\))用以获得representation. 最后得到的就是 \\[ Representation_{for\\ S_i} = softmax(\\boldsymbol{q}_{1\\times d_k}\\cdot \\boldsymbol K_{m \\times d_k}^T) \\boldsymbol V_{m\\times d_k} \\] 进一步可以获得 \\(T\\) 对 \\(S\\) 的表示 \\[ softmax(\\boldsymbol{Q}_{n\\times d_k}\\cdot \\boldsymbol K_{m \\times d_k}^T) \\boldsymbol V_{m\\times d_v} \\] 为了\"having more stable gradients\" , 在\\(\\boldsymbol{Q}\\cdot \\boldsymbol K^T\\)这里还要除以一个因子, 默认是 \\(\\sqrt{d_k}\\) , 然后又变成了 \\[ softmax(\\frac{\\boldsymbol{Q}\\cdot \\boldsymbol K^T}{\\sqrt{d_k}}) \\boldsymbol V \\] 有人要问了, 你说的这些query,key和value向量都咋求呢. 用三个线性映射(矩阵) \\(W^Q,W^K,W^V\\) 线性映射哪来的呢 学出来的 a beast with multihead 这样一组attention可能注意力太集中, 看不全, 那我们就让他有多个\"头\", 注意力分散点, 看得更全 tensor2tensor上有个示例, 演示的就是attention 一个比较经典的例子 翻译句子 The animal didn't cross the street because it was too tired 这里的 it 应该代指 The animal, 但是对模型来说, 他也可能是说the street. 这里可以看到, 模型确实被it的代指给弄晕了,但还好animal处的颜色比street的地方要深,说明他的权值要大 但并不是所有的attention都能学到对应的部分. 解决办法就是, 我们用多个attention去拼接成最终想要的representation. 具体的, 我们得到的value向量不是 \\(d_v\\) 维的吗, 假设我们有 \\(h\\) 个head, 那么就把向量分为\\(h\\) 个维度为 \\(d_v / h\\) 的向量, 每个用各自的线性映射得到 \\(h\\) 组不同的 \\(\\boldsymbol{Q,\\, K,\\, V_{m \\times (d_v/h)}}\\) 去求各自的value(attention结构图片来自这个blog) 最后一般还会再乘上一个矩阵 \\(W^O\\),来得到最后的输出 用上了多个head, 我们就能同时去关注不同的区域, 获得更准确的表述 整体架构 前面花了较长篇幅讲了Attention机制, 这里再看下它是如何被用在Transformer中的 在上面的架构图中包含encoder以及decoder的结构(结构图来自原论文) 左边是encoder, 他的特点就是直接将整条序列直接放进网络层中. 工作流程就是, 首先把要处理的序列input输入, 再获得它的embeddings. 然后依次进入每个encoder层,"},{"title":"我的第一篇文章","date":"2020-11-08T19:03:14.000Z","url":"/2020/11/08/%E6%88%91%E7%9A%84%E7%AC%AC%E4%B8%80%E7%AF%87%E6%96%87%E7%AB%A0/","tags":[["test","/tags/test/"]],"categories":[["test","/categories/test/"]],"content":"this is my first blog here"},{"title":"about","date":"2020-11-08T20:32:12.000Z","url":"/about/index.html","categories":[[" ",""]],"content":"Wu Shiguang. Taishan College, Shandong University"},{"title":"categories","date":"2020-11-09T01:39:56.000Z","url":"/categories/index.html","categories":[[" ",""]],"content":""},{"title":"search","date":"2020-11-08T20:14:33.000Z","url":"/search/index.html","categories":[[" ",""]],"content":""},{"title":"small talk","date":"2021-10-02T15:43:21.000Z","url":"/small-talk/index.html","categories":[[" ",""]],"content":""},{"title":"tags","date":"2020-11-08T20:30:26.000Z","url":"/tags/index.html","categories":[[" ",""]],"content":""}]